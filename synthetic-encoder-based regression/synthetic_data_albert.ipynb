{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQGHrD4GLd6B",
    "outputId": "03c467a7-b8d4-43da-de2f-848b28b43c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT5qZq2pFBGH"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0cT_KkUKype"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Seed all RNG sources so repeated runs stay deterministic.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H_20VJeqKynI"
   },
   "outputs": [],
   "source": [
    "TRAIN_JSON_PATH = \"/content/drive/MyDrive/nlp/train.json\"\n",
    "DEV_JSON_PATH   = \"/content/drive/MyDrive/nlp/dev.json\"\n",
    "SYN_JSON_PATH   = \"/content/drive/MyDrive/nlp/merged_updated.json\"\n",
    "MODEL_NAME = \"albert/albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zuh4fV-KylF"
   },
   "outputs": [],
   "source": [
    "def load_json_records(path, key=None):\n",
    "    \"\"\"Read a JSON file and optionally select a nested mapping by key.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if key:\n",
    "        data = data[key]\n",
    "    return list(data.values()) if isinstance(data, dict) else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Oq_qFlQK4tw"
   },
   "outputs": [],
   "source": [
    "def build_examples(records):\n",
    "    \"\"\"Convert raw sense annotations into Trainer-friendly dictionaries.\"\"\"\n",
    "\n",
    "    # Normalize: flatten nested lists so every item is a dict\n",
    "    def flatten(xs):\n",
    "        for x in xs:\n",
    "            if isinstance(x, list):\n",
    "                yield from flatten(x)\n",
    "            else:\n",
    "                yield x\n",
    "\n",
    "    # If a single dict is passed\n",
    "    if isinstance(records, dict):\n",
    "        records = [records]\n",
    "\n",
    "    # Flatten any nested structure\n",
    "    records = list(flatten(records))\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for r in records:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        pre = (r.get(\"precontext\") or \"\").strip()\n",
    "        sent = (r.get(\"sentence\") or \"\").strip()\n",
    "        end = (r.get(\"ending\") or \"\").strip()\n",
    "        meaning = (r.get(\"judged_meaning\") or \"\").strip()\n",
    "\n",
    "        meaning_txt = (\n",
    "            \"Meaning (intended definition):\\n\"\n",
    "            f\"{meaning}\"\n",
    "        )\n",
    "\n",
    "        # Build a single narrative block by concatenating story fragments.\n",
    "        story_parts = [pre, sent]\n",
    "        if end:\n",
    "            story_parts.append(end)\n",
    "\n",
    "        story_txt = \"Story:\\n\" + \"\\n\".join(story_parts)\n",
    "\n",
    "        ex_sent = (r.get(\"example_sentence\") or \"\").strip()\n",
    "        if ex_sent:\n",
    "            story_txt += f\"\\nExample sentence: {ex_sent}\"\n",
    "\n",
    "        label = float(r.get(\"average\", 0.0))\n",
    "\n",
    "        gid = f\"{r.get('homonym', '')}||{sent}\"\n",
    "\n",
    "        # Persist both model inputs and diagnostic metadata for later analysis.\n",
    "        out.append({\n",
    "            \"context\": story_txt,\n",
    "            \"meaning\": meaning_txt,\n",
    "            \"label\": label,\n",
    "            \"stdev\": float(r.get(\"stdev\", 0)),\n",
    "            \"group_id\": gid,\n",
    "            \"choices\": r.get(\"choices\"),\n",
    "            \"nonsensical\": r.get(\"nonsensical\"),\n",
    "            \"sample_id\": r.get(\"sample_id\")\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84OHzAJaK4rr",
    "outputId": "f4006650-2c44-43fb-96ac-b06f4c890ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2280\n",
      "DEV: 588\n",
      "SYN: 792\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON splits and normalize them into DataFrame form for inspection.\n",
    "train_records = load_json_records(TRAIN_JSON_PATH)\n",
    "dev_records   = load_json_records(DEV_JSON_PATH)\n",
    "syn_records   = load_json_records(SYN_JSON_PATH)\n",
    "train_df = pd.DataFrame(build_examples(train_records))\n",
    "dev_df   = pd.DataFrame(build_examples(dev_records))\n",
    "syn_df   = pd.DataFrame(build_examples(syn_records))\n",
    "print(\"TRAIN:\", len(train_df))\n",
    "print(\"DEV:\", len(dev_df))\n",
    "print(\"SYN:\", len(syn_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q7Clew4JwvB",
    "outputId": "3d3bab30-7751-4144-f79a-bff994fd5ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Story:\\nThe old machine hummed in the corner o...   \n",
      "1  Story:\\nThe old machine hummed in the corner o...   \n",
      "2  Story:\\nThe old machine hummed in the corner o...   \n",
      "3  Story:\\nThe old machine hummed in the corner o...   \n",
      "4  Story:\\nThe old machine hummed in the corner o...   \n",
      "\n",
      "                                             meaning  label     stdev  \\\n",
      "0  Meaning (intended definition):\\nthe difference...    3.0  1.581139   \n",
      "1  Meaning (intended definition):\\nthe inherent c...    3.8  0.836660   \n",
      "2  Meaning (intended definition):\\nthe difference...    2.2  1.303840   \n",
      "3  Meaning (intended definition):\\nthe inherent c...    4.4  0.894427   \n",
      "4  Meaning (intended definition):\\nthe difference...    2.6  1.516575   \n",
      "\n",
      "                                         group_id          choices  \\\n",
      "0  potential||The potential couldn't be measured.  [4, 5, 2, 3, 1]   \n",
      "1  potential||The potential couldn't be measured.  [5, 3, 4, 4, 3]   \n",
      "2  potential||The potential couldn't be measured.  [2, 1, 4, 3, 1]   \n",
      "3  potential||The potential couldn't be measured.  [4, 5, 5, 3, 5]   \n",
      "4  potential||The potential couldn't be measured.  [1, 1, 4, 4, 3]   \n",
      "\n",
      "                           nonsensical sample_id  \n",
      "0  [False, False, False, False, False]      1843  \n",
      "1  [False, False, False, False, False]      1844  \n",
      "2  [False, False, False, False, False]      1845  \n",
      "3  [False, False, False, False, False]      1846  \n",
      "4  [False, False, False, False, False]      1847  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX9cjEyOHSgF",
    "outputId": "2090d891-8f04-409d-fd9a-c63394acbad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 3072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Augment the supervised data with the synthetic examples before splitting.\n",
    "train_df = pd.concat([train_df, syn_df])\n",
    "print(\"TRAIN:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67pFeacjK4pV",
    "outputId": "af4cd113-547c-4b4d-8863-f5b8a82aed55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2457 VAL: 615\n"
     ]
    }
   ],
   "source": [
    "# Hold out 20% of the blended data for validation during training.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAIN:\", len(train_df), \"VAL:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "c84e6443e8a3427aae48fb4bad229ada",
      "92289b7383934d4aa419b6a10133fad1",
      "23ea70f8d3ed4c77ae2d50f8ad389455",
      "34d60dc50c4340a2ab59df518882a6a0",
      "0024b03cc03a4e54b460392e7dbe42bb",
      "24c1db363cce4d7684573b2175a06fc1",
      "34f707bb04ff4ba693e74b0c348d54d2",
      "f3dd022813f049859e0280b0e8e8a547",
      "8119beb0d04943f983b06d1b27b2a4bf",
      "cb5b26c177924e4ba400eae64ce7a405",
      "1d9c0b184e92474ab30e46b969eeed87",
      "93166140f501461ba79284953b944724",
      "2b7f7d71e220439ab050d5b18179ac96",
      "f7f50148b1e946a29895ac87d00dc287",
      "17136e570ec74506b2573e17b9a6bc32",
      "59cd1fbe7be24710811e48ad4e5a861d",
      "a48330305fa8487f9bfb15d3db51baf6",
      "fe04f72a523a4ccabda374ffbf639a84",
      "ad0c13b831184bd6a66eb257cb48cac9",
      "6fff7a0eb69b40018d1b6e556627be61",
      "4ec240adf9e4493ea888ea17d1012f75",
      "175d404cc91f491e85af32603f93cf27",
      "c8b29a4255a14642bf2926353e4dbd67",
      "1785e533fab04864b8b2d3b59d3c0e10",
      "7357f9b8747348ef84d4a3a0d6bfc8a5",
      "bcedcb3a7bf644beb6bde42ce8a30e55",
      "2470cb4e37f64d68b3cfcaaf79a15e4f",
      "fde55af6431246b5bfaea9393cf1d061",
      "f42157668e3b4073a919c2c17e6a78ab",
      "f0fbf3307a004295bddb957e8d766781",
      "08554dd2228749ea9d410270f14844ae",
      "72cfaec785c74f88a9899f033551ae89",
      "438f774eac314a4e8940ba671ebd9083",
      "dafc75d5af404f478236e34d398b9c01",
      "b0b2ff66e03f4510811f0929ca39e149",
      "9323f8173e064fb39d97ab2c705105bd",
      "123ff6f293d34032868bd70555d71be3",
      "8a4f0cf32ff44b7dbb339b01ae59dcc9",
      "e735a2a53df84abfac07ff8a7e9d0626",
      "63f9045c3e42436eb4fd5c96212de4eb",
      "25e6198d3df84347a197cdbeb6419218",
      "1918dfee8d4a4c488f1d6885373063b3",
      "00cd538a5fee4c8593d4fa1e647ea812",
      "af924f53b4f04cafb2b52e23af3c33a3"
     ]
    },
    "id": "RjpSLkDhK4nB",
    "outputId": "e16efe2f-cdde-46cd-d629-693851fa8ab5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e6443e8a3427aae48fb4bad229ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93166140f501461ba79284953b944724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b29a4255a14642bf2926353e4dbd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafc75d5af404f478236e34d398b9c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    \"\"\"Pair the story and meaning text and attach the regression label.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch[\"context\"],\n",
    "        batch[\"meaning\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"label\"]\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "bd867aecff4b445f9ee31f6200a12199",
      "dedfe08f2401446da20d619597b25e08",
      "434dfb522657413081689a50dacffaef",
      "928bb33c2cc14c3789f69e704db14ad7",
      "c44c78639cb845b0858d6e1f8dcd4122",
      "4bebcfe1ff9c4f26b3370198e45ad018",
      "a549fe8ec98f4d6088de0b13b42fb943",
      "211135435be0436ca2e1dbf2f7e4bd5e",
      "d41297aa412345ce8d13bc0bd4bc3a14",
      "73756c9144674eb5b0232012d06a2de5",
      "9b49bcf4d2d440e6afcd73af61f37467",
      "c567588411094d2eae90f6ed91bee03a",
      "779d13de0c2f41f6bf37dd02c048e2bb",
      "2e96cb96c08e40668b3ad2866127083a",
      "1cc800a74577436b99877a58c3c11154",
      "23576622b2f5483aa62dfa6956179ac6",
      "9352321b64c84efa9759710b5c9a5d1d",
      "16a27dbd6991430498d7d8c722c08aee",
      "3a8cc75fc2084b818a42c08020d5a1bc",
      "30f6d561ca714facb67209f9e26562a2",
      "e82cc5a909a840f9b225f8b243f5e154",
      "1f08988a5252453db782daf58547a6ca",
      "5f08149c716345c2a3dec731edc8eb9c",
      "356b5ade05ab42418620f82d6bad5977",
      "f128661c47ae45ffa5719c4df1a8398a",
      "9d41b1ea736141ac80f8d52c2d61bfec",
      "88e59581a13e4439b6ac5f5ee58e0783",
      "c56ae28632954b6bbca1ee46e9dea4c1",
      "3272011e54a84fadb222f05fadbfefff",
      "eca70a5db8ac4c9c8f91c91c2b6e8d85",
      "22c7553d2f894b23a1872a4180b1295e",
      "79d8dd12f3494afeabcc3a2489834bf8",
      "7ba674ba376648e5a3d986b4d2dab96f"
     ]
    },
    "id": "u7B64q6ELAhf",
    "outputId": "1dfa1f07-a22b-4223-f70b-932110e5b166"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd867aecff4b445f9ee31f6200a12199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c567588411094d2eae90f6ed91bee03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f08149c716345c2a3dec731edc8eb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Materialize Hugging Face Datasets and apply the tokenizer once up front.\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "val_dataset   = Dataset.from_pandas(val_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df, preserve_index=False).map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "9b41c2b9926c48209ef2a948e51c2534",
      "9def1cfb3b7f4bd085ba54e96d74abd5",
      "7a703ff7f13b4198b2dced1efaed076d",
      "4adfd986f7ab4205ba2174f47d200cde",
      "053d460d1a524dfba47c7653025a1083",
      "93774e9cae4f45e98a366fb77de49d64",
      "2937f7dc8b9c4390aca9525b293fbad5",
      "a397c32513434f1cadfffa329b9f7bd5",
      "5260065964cc46bf959ee07124581da2",
      "93b71b91823b48dbae564d68775e67ba",
      "413c3c733373408fa443de90cf6fc50a"
     ]
    },
    "id": "M9ZJQKEALATb",
    "outputId": "a2c3f40c-8d9e-40f6-abe0-7dcc361d1e0a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b41c2b9926c48209ef2a948e51c2534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKw-lMXRLARn"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Evaluate rank correlation plus absolute error for the regression task.\"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.reshape(-1)\n",
    "\n",
    "    sp = spearmanr(labels, preds).correlation\n",
    "    mae = np.mean(np.abs(labels - preds))\n",
    "\n",
    "    return {\"spearman\": float(sp), \"mae\": float(mae)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5HUW9saLIt-"
   },
   "outputs": [],
   "source": [
    "# Trainer hyperparameters chosen for a lightweight regression run on Colab.\n",
    " args = TrainingArguments(\n",
    "    output_dir=\"./tmp_albert_regression\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEf0dpC-LIqV",
    "outputId": "19b5cd25-31e7-40aa-a3a7-7a45611ff82b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3517603358.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "b9j3fAOmLLQP",
    "outputId": "a7eaa7ac-0e0c-403f-f81d-54dfa0b4ab6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1540' max='1540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1540/1540 01:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.263600</td>\n",
       "      <td>1.370981</td>\n",
       "      <td>0.353771</td>\n",
       "      <td>0.977977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.976948</td>\n",
       "      <td>0.555098</td>\n",
       "      <td>0.795083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.622200</td>\n",
       "      <td>0.807775</td>\n",
       "      <td>0.599922</td>\n",
       "      <td>0.706449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.798149</td>\n",
       "      <td>0.637867</td>\n",
       "      <td>0.693122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.791549</td>\n",
       "      <td>0.663272</td>\n",
       "      <td>0.677821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.773682</td>\n",
       "      <td>0.659285</td>\n",
       "      <td>0.682463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.710679</td>\n",
       "      <td>0.679105</td>\n",
       "      <td>0.660205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.726749</td>\n",
       "      <td>0.674967</td>\n",
       "      <td>0.656141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.735972</td>\n",
       "      <td>0.670851</td>\n",
       "      <td>0.663574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.676020</td>\n",
       "      <td>0.657223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1540, training_loss=0.4786604423027534, metrics={'train_runtime': 95.9543, 'train_samples_per_second': 256.06, 'train_steps_per_second': 16.049, 'total_flos': 293559158430720.0, 'train_loss': 0.4786604423027534, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "SWY7VsQNLLNv",
    "outputId": "58079f63-6a7e-4483-e11f-065c66d69156"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEV EVALUATION ===\n",
      "Spearman: 0.4205\n",
      "MAE:      0.8828\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(dev_dataset).predictions.reshape(-1)\n",
    "dev_labels = dev_df[\"label\"].to_numpy()\n",
    "\n",
    "global_spearman = spearmanr(dev_labels, preds).correlation\n",
    "mae = np.mean(np.abs(dev_labels - preds))\n",
    "\n",
    "print(f\"\\n=== DEV EVALUATION ===\")\n",
    "print(f\"Spearman: {global_spearman:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEg2ZzvSN7j2"
   },
   "outputs": [],
   "source": [
    "def bootstrap_test_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_stdev,\n",
    "    groups,\n",
    "    n_bootstrap=1000,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"Estimate confidence intervals for each metric via bootstrap resampling.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y_true)\n",
    "\n",
    "    global_sps = []\n",
    "    macro_sps = []\n",
    "    maes = []\n",
    "    acc_stdevs = []\n",
    "\n",
    "    group_to_indices = defaultdict(list)\n",
    "    for i, gid in enumerate(groups):\n",
    "        group_to_indices[gid].append(i)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample indices with replacement to simulate new draws from the corpus.\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "\n",
    "        bt_true = y_true[idx]\n",
    "        bt_pred = y_pred[idx]\n",
    "        bt_stdev = y_stdev[idx]\n",
    "        bt_groups = [groups[i] for i in idx]\n",
    "\n",
    "        # ---- Global Spearman ----\n",
    "        rho = spearmanr(bt_true, bt_pred).correlation\n",
    "        global_sps.append(rho)\n",
    "\n",
    "        # ---- MAE ----\n",
    "        maes.append(np.mean(np.abs(bt_true - bt_pred)))\n",
    "\n",
    "        # ---- Accuracy within stdev ----\n",
    "        acc_stdevs.append(np.mean(np.abs(bt_true - bt_pred) <= bt_stdev))\n",
    "\n",
    "        # ---- Macro Spearman ----\n",
    "        local_group_map = defaultdict(list)\n",
    "        for i, g in enumerate(bt_groups):\n",
    "            local_group_map[g].append(i)\n",
    "\n",
    "        group_corrs = []\n",
    "        for g, idxs in local_group_map.items():\n",
    "            gt = bt_true[idxs]\n",
    "            gp = bt_pred[idxs]\n",
    "\n",
    "            if np.all(gt == gt[0]):\n",
    "                continue\n",
    "\n",
    "            c = spearmanr(gt, gp).correlation\n",
    "            if not np.isnan(c):\n",
    "                group_corrs.append(c)\n",
    "\n",
    "        if group_corrs:\n",
    "            macro_sps.append(np.mean(group_corrs))\n",
    "        else:\n",
    "            macro_sps.append(np.nan)\n",
    "\n",
    "    def summarize(arr):\n",
    "        # Report both point estimate (mean) and 95% percentile interval.\n",
    "        arr = np.array(arr, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.nanmean(arr)),\n",
    "            \"ci_low\": float(np.nanpercentile(arr, 2.5)),\n",
    "            \"ci_high\": float(np.nanpercentile(arr, 97.5)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"global_spearman\": summarize(global_sps),\n",
    "        \"macro_spearman\": summarize(macro_sps),\n",
    "        \"mae\": summarize(maes),\n",
    "        \"acc_within_stdev\": summarize(acc_stdevs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw7WwJuYOAsf",
    "outputId": "6cf56797-080a-4b44-9ded-4aeecd404ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOOTSTRAP RESULTS (TEST SET)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3268535183.py:52: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  c = spearmanr(gt, gp).correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_spearman     : 0.4210 [0.3517, 0.4850]\n",
      "macro_spearman      : 0.3841 [0.2965, 0.4682]\n",
      "mae                 : 0.8825 [0.8306, 0.9419]\n",
      "acc_within_stdev    : 0.5935 [0.5544, 0.6327]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBOOTSTRAP RESULTS (TEST SET)\")\n",
    "\n",
    "bootstrap_results = bootstrap_test_metrics(\n",
    "    y_true=dev_df[\"label\"].to_numpy(),\n",
    "    y_pred=preds,\n",
    "    y_stdev=dev_df[\"stdev\"].to_numpy(),\n",
    "    groups=dev_df[\"group_id\"].tolist(),\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "\n",
    "for metric, stats in bootstrap_results.items():\n",
    "    print(\n",
    "        f\"{metric:20s}: \"\n",
    "        f\"{stats['mean']:.4f} \"\n",
    "        f\"[{stats['ci_low']:.4f}, {stats['ci_high']:.4f}]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
