{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQGHrD4GLd6B",
    "outputId": "05e624af-8772-4c0d-f662-f9bf9666ac6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aT5qZq2pFBGH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0cT_KkUKype"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Seed Python, NumPy, Torch, and HF so results are reproducible.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H_20VJeqKynI"
   },
   "outputs": [],
   "source": [
    "TRAIN_JSON_PATH = \"/content/drive/MyDrive/nlp/train.json\"\n",
    "DEV_JSON_PATH   = \"/content/drive/MyDrive/nlp/dev.json\"\n",
    "SYN_JSON_PATH   = \"/content/drive/MyDrive/nlp/merged_updated.json\"\n",
    "MODEL_NAME = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zuh4fV-KylF"
   },
   "outputs": [],
   "source": [
    "def load_json_records(path, key=None):\n",
    "    \"\"\"Load a JSON file and optionally drill into a nested mapping.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if key:\n",
    "        data = data[key]\n",
    "    return list(data.values()) if isinstance(data, dict) else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Oq_qFlQK4tw"
   },
   "outputs": [],
   "source": [
    "def build_examples(records):\n",
    "    \"\"\"Convert raw annotation dicts into model-ready context/meaning pairs.\"\"\"\n",
    "    # Normalize: flatten nested lists so every item is a dict\n",
    "    def flatten(xs):\n",
    "        for x in xs:\n",
    "            if isinstance(x, list):\n",
    "                yield from flatten(x)\n",
    "            else:\n",
    "                yield x\n",
    "\n",
    "    # If a single dict is passed\n",
    "    if isinstance(records, dict):\n",
    "        records = [records]\n",
    "\n",
    "    # Flatten any nested structure\n",
    "    records = list(flatten(records))\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for r in records:\n",
    "        # Skip items that are not dicts (extra safety)\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        pre = (r.get(\"precontext\") or \"\").strip()\n",
    "        sent = (r.get(\"sentence\") or \"\").strip()\n",
    "        end = (r.get(\"ending\") or \"\").strip()\n",
    "        meaning = (r.get(\"judged_meaning\") or \"\").strip()\n",
    "\n",
    "        meaning_txt = (\n",
    "            \"Meaning (intended definition):\\n\"\n",
    "            f\"{meaning}\"\n",
    "        )\n",
    "\n",
    "        # Merge the narrative fragments into a single multi-line prompt.\n",
    "        story_parts = [pre, sent]\n",
    "        if end:\n",
    "            story_parts.append(end)\n",
    "\n",
    "        story_txt = \"Story:\\n\" + \"\\n\".join(story_parts)\n",
    "\n",
    "        ex_sent = (r.get(\"example_sentence\") or \"\").strip()\n",
    "        if ex_sent:\n",
    "            story_txt += f\"\\nExample sentence: {ex_sent}\"\n",
    "\n",
    "        label = float(r.get(\"average\", 0.0))\n",
    "\n",
    "        gid = f\"{r.get('homonym', '')}||{sent}\"\n",
    "\n",
    "        # Package both supervision signal and diagnostics for later eval.\n",
    "        out.append({\n",
    "            \"context\": story_txt,\n",
    "            \"meaning\": meaning_txt,\n",
    "            \"label\": label,\n",
    "            \"stdev\": float(r.get(\"stdev\", 0)),\n",
    "            \"group_id\": gid,\n",
    "            \"choices\": r.get(\"choices\"),\n",
    "            \"nonsensical\": r.get(\"nonsensical\"),\n",
    "            \"sample_id\": r.get(\"sample_id\")\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84OHzAJaK4rr",
    "outputId": "6426a560-3f1f-4b5f-b848-21acb2c07ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2280\n",
      "DEV: 588\n",
      "SYN: 792\n"
     ]
    }
   ],
   "source": [
    "# Load each split and normalize the nested JSON structure into DataFrames.\n",
    "train_records = load_json_records(TRAIN_JSON_PATH)\n",
    "dev_records   = load_json_records(DEV_JSON_PATH)\n",
    "syn_records   = load_json_records(SYN_JSON_PATH)\n",
    "train_df = pd.DataFrame(build_examples(train_records))\n",
    "dev_df   = pd.DataFrame(build_examples(dev_records))\n",
    "syn_df   = pd.DataFrame(build_examples(syn_records))\n",
    "print(\"TRAIN:\", len(train_df))\n",
    "print(\"DEV:\", len(dev_df))\n",
    "print(\"SYN:\", len(syn_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q7Clew4JwvB",
    "outputId": "092bde5e-c5d6-49c2-e319-bdabc4a3a488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Story:\\nThe old machine hummed in the corner o...   \n",
      "1  Story:\\nThe old machine hummed in the corner o...   \n",
      "2  Story:\\nThe old machine hummed in the corner o...   \n",
      "3  Story:\\nThe old machine hummed in the corner o...   \n",
      "4  Story:\\nThe old machine hummed in the corner o...   \n",
      "\n",
      "                                             meaning  label     stdev  \\\n",
      "0  Meaning (intended definition):\\nthe difference...    3.0  1.581139   \n",
      "1  Meaning (intended definition):\\nthe inherent c...    3.8  0.836660   \n",
      "2  Meaning (intended definition):\\nthe difference...    2.2  1.303840   \n",
      "3  Meaning (intended definition):\\nthe inherent c...    4.4  0.894427   \n",
      "4  Meaning (intended definition):\\nthe difference...    2.6  1.516575   \n",
      "\n",
      "                                         group_id          choices  \\\n",
      "0  potential||The potential couldn't be measured.  [4, 5, 2, 3, 1]   \n",
      "1  potential||The potential couldn't be measured.  [5, 3, 4, 4, 3]   \n",
      "2  potential||The potential couldn't be measured.  [2, 1, 4, 3, 1]   \n",
      "3  potential||The potential couldn't be measured.  [4, 5, 5, 3, 5]   \n",
      "4  potential||The potential couldn't be measured.  [1, 1, 4, 4, 3]   \n",
      "\n",
      "                           nonsensical sample_id  \n",
      "0  [False, False, False, False, False]      1843  \n",
      "1  [False, False, False, False, False]      1844  \n",
      "2  [False, False, False, False, False]      1845  \n",
      "3  [False, False, False, False, False]      1846  \n",
      "4  [False, False, False, False, False]      1847  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX9cjEyOHSgF",
    "outputId": "aad3deb9-1a50-48b5-aed6-f4b36ed6f000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 3072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenating the real and synthetic data before the downstream split.\n",
    "train_df = pd.concat([train_df, syn_df])\n",
    "print(\"TRAIN:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67pFeacjK4pV",
    "outputId": "e49661a0-24e1-4d19-8c3d-b25fb85a2248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2457 VAL: 615\n"
     ]
    }
   ],
   "source": [
    "# Reserve 20% of the combined corpus for live validation during training.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAIN:\", len(train_df), \"VAL:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "f41eb0f8afb74e77ae34391044529f26",
      "65fce1e1d74c4f9b9311f7e13825d9b8",
      "93f2e4cbc0d9489991b37a309ece314e",
      "b77c342717274667900ea55fcbf13890",
      "7779dbf2efbb4bf9aea7cb5f598d569e",
      "c19f27638b1b4d81b9537db28ea534f6",
      "15fdd05e14f54cd789e72365e527caee",
      "9cbdbe02399043a0960070f848de4b51",
      "766e412138db41fa9a22d878101d7a07",
      "7137e6a652534077967fc2baaa463147",
      "6fb8eae151f146469dd075bba624e807",
      "6f2e48909b1c4267aad5a880565f25f6",
      "47da2047239d4753a2ba585767a75ea2",
      "65a1ffd8bfac498c85c37378c6db6a45",
      "9cc7093d0b0e4869b2c0376be1eca576",
      "a3deeb3880fa498a804f4f2529c51911",
      "54d8f742dbf2469888a3c995b00445dc",
      "9c24dd0d399b406285eca8a7caba6115",
      "a5fde11f6de24776beee54153cca0770",
      "0bff8fbad742452cb7c8460c8c7fb629",
      "2d2e612865024ebfb17ef73ff3e9fa68",
      "416bf17eb045465d8761f0f951786513",
      "85ca1e68971d4b95b3e50f443371694d",
      "a76fe84ae08d4ea184203f65a0ce75bc",
      "7184551e22af4661a3fc5cf95b7bcd29",
      "7bbd5bec663d40e98a0d86da692088e0",
      "b570992541dc4eb88083c5b2675fb8c2",
      "580cacceb6ae45128de36b355321092d",
      "874adde444474ed7b40dadd7d69ca16a",
      "9ca762be4fcb440aa77c673684dbb133",
      "9ea67684167846e3a1aa7394a5d6081c",
      "74995fd335ed452688ca2d542fcd5bc0",
      "1986f916675a422e83c75eb6a0f4d750",
      "c9ccaf9d3ff44c4b81fad604a2ff12fb",
      "a8fed72ae9c6408c81a973f569d33dc0",
      "f77c0e9111d7450a8530afa43f9f19f6",
      "8e5d2a631f4249329fe6bd1d94b273ef",
      "b1b0d6186b234b7ba56e4f6ae11cb055",
      "74739a77d24244248576c7a60c81896b",
      "80d2bd7810c24198b0ddfb05fd4a07cb",
      "654747e40cca43dfae46782f94e651bc",
      "576c97a2c9714c7499a2b784ce021397",
      "e96b6c41271645bcaf599464c33df7d8",
      "6278e76113904d10927ea01d3df0263a"
     ]
    },
    "id": "RjpSLkDhK4nB",
    "outputId": "dc52b320-3aca-407a-c529-7c605b658986"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41eb0f8afb74e77ae34391044529f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2e48909b1c4267aad5a880565f25f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ca1e68971d4b95b3e50f443371694d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ccaf9d3ff44c4b81fad604a2ff12fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    \"\"\"Tokenize paired inputs and thread the regression label through.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch[\"context\"],\n",
    "        batch[\"meaning\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"label\"]\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "ccc12e1f9d3c4bf38fd40c55ffea734f",
      "e73f2a1d01ee444397afd9307b0603c7",
      "2f73308399d144ab9fa175934f31b5e1",
      "6e80bad49a964dd586454f75337353ec",
      "554ec9f046c64c9aa8b37cbd89a24fbe",
      "7fa4af45437d43839ff24b28f90d45e2",
      "ffd5a611427b47deb1c58ae7a943aa25",
      "940c326cf1f74b14aa17d3e0978d8da7",
      "7c627d25a8834a2e9a5db45e7c323942",
      "16286f05025244f58f849dbf44aa523f",
      "f7106ae6c1ce4a94a33fd24affd4b203",
      "59798f4856524f41ad6a9c4141b49f1e",
      "cd2b6f271e0849038ae58e5565e179ea",
      "3b98b37c5e474ba38e32576cd3b69a50",
      "e4914019f2724765885c9759557e9e92",
      "e8f2df8478e54a09a4402f012c705e4a",
      "f399fb04328e4bb18511603ccf243e6c",
      "5784175718b8459a8ccab31399d20330",
      "41d2fa4d59454bf5a32b54ee4e2d6341",
      "b965362b0f884cb2b0226bd161f59761",
      "e3fe23bb04574443b4594546ef4b0489",
      "3611bc6e8790471b9dfe127a2a4fba03",
      "d5046210434c4a5e882c22f52fa1b67c",
      "0bb0e57816484c19a778a97683993373",
      "d1a9cedb553b473aa64d3cf5d9171a5b",
      "d1480608b6d94a83860445a4296e28be",
      "2d900272e99f49849f69f4e1d3076d94",
      "c234861c0ed64c048a219efa217c80d0",
      "526088503e0a46b5bf55c9a8df819181",
      "c8bfc2c6163547938a11e61c41fb2dd2",
      "89b83086555441c7bddaa3582f94596b",
      "5955f57818614a3183beec2fd5f51dbc",
      "3125baaa50da407b9108868218cc721e"
     ]
    },
    "id": "u7B64q6ELAhf",
    "outputId": "36cc3567-839e-4056-be0e-2f4d6f6f27d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc12e1f9d3c4bf38fd40c55ffea734f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59798f4856524f41ad6a9c4141b49f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5046210434c4a5e882c22f52fa1b67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-tokenize each split once so the Trainer can stream batches efficiently.\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "val_dataset   = Dataset.from_pandas(val_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df, preserve_index=False).map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "d6e94cbde51c44928582161c0d321868",
      "0ec511de1aa848f0b279ef3d8b92e07b",
      "ba06f25f69f44138812e02f7e026afe2",
      "f59f768afdda4ccc99669ac16fff61cd",
      "36dc93d4d40c490f8c276ae1e8bac7c4",
      "d128a66f925f4381a5da2e14aa23efe8",
      "45b2ae8dd6e74b09ac97a41eac56d309",
      "ae077095aec1499e9bd023a4d50c47a2",
      "3023f8142911495bae3ef01da06d78a6",
      "5a1e42c8a0784235afd84d280cea7bfc",
      "f957807b15cb44d8b8ae05227f45b5d9"
     ]
    },
    "id": "M9ZJQKEALATb",
    "outputId": "d0a28608-eb93-4824-e7a2-f59d8a066820"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e94cbde51c44928582161c0d321868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKw-lMXRLARn"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Report both rank correlation and absolute error for dev evals.\"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.reshape(-1)\n",
    "\n",
    "    sp = spearmanr(labels, preds).correlation\n",
    "    mae = np.mean(np.abs(labels - preds))\n",
    "\n",
    "    return {\"spearman\": float(sp), \"mae\": float(mae)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5HUW9saLIt-"
   },
   "outputs": [],
   "source": [
    "# hyperparameters for finetuning base BERT on Colab GPUs.\n",
    " args = TrainingArguments(\n",
    "    output_dir=\"./tmp_bert_regression\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",   # you requested: no saving\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEf0dpC-LIqV",
    "outputId": "933c2b29-791f-4f1a-ebd6-d2b3bf15a4f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3517603358.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "b9j3fAOmLLQP",
    "outputId": "a773e2d4-1c59-464b-81af-03dbcf654709"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1540' max='1540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1540/1540 01:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.363600</td>\n",
       "      <td>1.467813</td>\n",
       "      <td>0.066362</td>\n",
       "      <td>1.026510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>1.147097</td>\n",
       "      <td>0.408946</td>\n",
       "      <td>0.877960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>1.029117</td>\n",
       "      <td>0.499060</td>\n",
       "      <td>0.806140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.627300</td>\n",
       "      <td>0.987050</td>\n",
       "      <td>0.541443</td>\n",
       "      <td>0.793409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.543800</td>\n",
       "      <td>1.003773</td>\n",
       "      <td>0.570174</td>\n",
       "      <td>0.773804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>1.008755</td>\n",
       "      <td>0.576878</td>\n",
       "      <td>0.776462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.996198</td>\n",
       "      <td>0.584073</td>\n",
       "      <td>0.767370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>1.003325</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.767258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.963182</td>\n",
       "      <td>0.597413</td>\n",
       "      <td>0.758425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.994619</td>\n",
       "      <td>0.599055</td>\n",
       "      <td>0.763942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1540, training_loss=0.6510652282021262, metrics={'train_runtime': 92.5815, 'train_samples_per_second': 265.388, 'train_steps_per_second': 16.634, 'total_flos': 3232290293406720.0, 'train_loss': 0.6510652282021262, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "SWY7VsQNLLNv",
    "outputId": "2849c9d0-9ea7-4312-92b6-627fbcbef90f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEV EVALUATION ===\n",
      "Spearman: 0.3355\n",
      "MAE:      1.0143\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(dev_dataset).predictions.reshape(-1)\n",
    "dev_labels = dev_df[\"label\"].to_numpy()\n",
    "\n",
    "global_spearman = spearmanr(dev_labels, preds).correlation\n",
    "mae = np.mean(np.abs(dev_labels - preds))\n",
    "\n",
    "print(f\"\\n=== DEV EVALUATION ===\")\n",
    "print(f\"Spearman: {global_spearman:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEg2ZzvSN7j2"
   },
   "outputs": [],
   "source": [
    "def bootstrap_test_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_stdev,\n",
    "    groups,\n",
    "    n_bootstrap=1000,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"Bootstrap confidence intervals for each metric to gauge stability.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y_true)\n",
    "\n",
    "    global_sps = []\n",
    "    macro_sps = []\n",
    "    maes = []\n",
    "    acc_stdevs = []\n",
    "\n",
    "    group_to_indices = defaultdict(list)\n",
    "    for i, gid in enumerate(groups):\n",
    "        group_to_indices[gid].append(i)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample indices with replacement to simulate new draws.\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "\n",
    "        bt_true = y_true[idx]\n",
    "        bt_pred = y_pred[idx]\n",
    "        bt_stdev = y_stdev[idx]\n",
    "        bt_groups = [groups[i] for i in idx]\n",
    "\n",
    "        # ---- Global Spearman ----\n",
    "        rho = spearmanr(bt_true, bt_pred).correlation\n",
    "        global_sps.append(rho)\n",
    "\n",
    "        # ---- MAE ----\n",
    "        maes.append(np.mean(np.abs(bt_true - bt_pred)))\n",
    "\n",
    "        # ---- Accuracy within stdev ----\n",
    "        acc_stdevs.append(np.mean(np.abs(bt_true - bt_pred) <= bt_stdev))\n",
    "\n",
    "        # ---- Macro Spearman ----\n",
    "        local_group_map = defaultdict(list)\n",
    "        for i, g in enumerate(bt_groups):\n",
    "            local_group_map[g].append(i)\n",
    "\n",
    "        group_corrs = []\n",
    "        for g, idxs in local_group_map.items():\n",
    "            gt = bt_true[idxs]\n",
    "            gp = bt_pred[idxs]\n",
    "\n",
    "            if np.all(gt == gt[0]):\n",
    "                continue\n",
    "\n",
    "            c = spearmanr(gt, gp).correlation\n",
    "            if not np.isnan(c):\n",
    "                group_corrs.append(c)\n",
    "\n",
    "        if group_corrs:\n",
    "            macro_sps.append(np.mean(group_corrs))\n",
    "        else:\n",
    "            macro_sps.append(np.nan)\n",
    "\n",
    "    def summarize(arr):\n",
    "        # Package mean estimates and 95% percentile-based confidence intervals.\n",
    "        arr = np.array(arr, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.nanmean(arr)),\n",
    "            \"ci_low\": float(np.nanpercentile(arr, 2.5)),\n",
    "            \"ci_high\": float(np.nanpercentile(arr, 97.5)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"global_spearman\": summarize(global_sps),\n",
    "        \"macro_spearman\": summarize(macro_sps),\n",
    "        \"mae\": summarize(maes),\n",
    "        \"acc_within_stdev\": summarize(acc_stdevs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw7WwJuYOAsf",
    "outputId": "e8934ea0-6724-4b90-fdd9-7ceceda2c243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOOTSTRAP RESULTS (TEST SET)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3268535183.py:52: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  c = spearmanr(gt, gp).correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_spearman     : 0.3366 [0.2629, 0.4040]\n",
      "macro_spearman      : 0.3379 [0.2403, 0.4265]\n",
      "mae                 : 1.0145 [0.9521, 1.0745]\n",
      "acc_within_stdev    : 0.5502 [0.5085, 0.5918]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBOOTSTRAP RESULTS (TEST SET)\")\n",
    "\n",
    "bootstrap_results = bootstrap_test_metrics(\n",
    "    y_true=dev_df[\"label\"].to_numpy(),\n",
    "    y_pred=preds,\n",
    "    y_stdev=dev_df[\"stdev\"].to_numpy(),\n",
    "    groups=dev_df[\"group_id\"].tolist(),\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "\n",
    "for metric, stats in bootstrap_results.items():\n",
    "    print(\n",
    "        f\"{metric:20s}: \"\n",
    "        f\"{stats['mean']:.4f} \"\n",
    "        f\"[{stats['ci_low']:.4f}, {stats['ci_high']:.4f}]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
