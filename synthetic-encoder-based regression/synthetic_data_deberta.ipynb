{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQGHrD4GLd6B",
    "outputId": "9d90d924-a9c0-4115-a36a-a240a078ede6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aT5qZq2pFBGH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0cT_KkUKype"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Stabilize Python/NumPy/Torch randomness plus Transformers seed.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H_20VJeqKynI"
   },
   "outputs": [],
   "source": [
    "TRAIN_JSON_PATH = \"/content/drive/MyDrive/nlp/train.json\"\n",
    "DEV_JSON_PATH   = \"/content/drive/MyDrive/nlp/dev.json\"\n",
    "SYN_JSON_PATH   = \"/content/drive/MyDrive/nlp/merged_updated.json\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zuh4fV-KylF"
   },
   "outputs": [],
   "source": [
    "def load_json_records(path, key=None):\n",
    "    \"\"\"Load JSON annotations and optionally unwrap a nested key.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if key:\n",
    "        data = data[key]\n",
    "    return list(data.values()) if isinstance(data, dict) else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Oq_qFlQK4tw"
   },
   "outputs": [],
   "source": [
    "def build_examples(records):\n",
    "    \"\"\"Flatten raw JSON structures into plain dicts Trainer can digest.\"\"\"\n",
    "    # Normalize: flatten nested lists so every item is a dict\n",
    "    def flatten(xs):\n",
    "        for x in xs:\n",
    "            if isinstance(x, list):\n",
    "                yield from flatten(x)\n",
    "            else:\n",
    "                yield x\n",
    "\n",
    "    # If a single dict is passed\n",
    "    if isinstance(records, dict):\n",
    "        records = [records]\n",
    "\n",
    "    # Flatten any nested structure\n",
    "    records = list(flatten(records))\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for r in records:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        pre = (r.get(\"precontext\") or \"\").strip()\n",
    "        sent = (r.get(\"sentence\") or \"\").strip()\n",
    "        end = (r.get(\"ending\") or \"\").strip()\n",
    "        meaning = (r.get(\"judged_meaning\") or \"\").strip()\n",
    "\n",
    "        meaning_txt = (\n",
    "            \"Meaning (intended definition):\\n\"\n",
    "            f\"{meaning}\"\n",
    "        )\n",
    "\n",
    "        # Build a readable story block to pair with the definition prompt.\n",
    "        story_parts = [pre, sent]\n",
    "        if end:\n",
    "            story_parts.append(end)\n",
    "\n",
    "        story_txt = \"Story:\\n\" + \"\\n\".join(story_parts)\n",
    "\n",
    "        ex_sent = (r.get(\"example_sentence\") or \"\").strip()\n",
    "        if ex_sent:\n",
    "            story_txt += f\"\\nExample sentence: {ex_sent}\"\n",
    "\n",
    "        label = float(r.get(\"average\", 0.0))\n",
    "\n",
    "        gid = f\"{r.get('homonym', '')}||{sent}\"\n",
    "\n",
    "        # Carry extra metadata to help downstream error analysis.\n",
    "        out.append({\n",
    "            \"context\": story_txt,\n",
    "            \"meaning\": meaning_txt,\n",
    "            \"label\": label,\n",
    "            \"stdev\": float(r.get(\"stdev\", 0)),\n",
    "            \"group_id\": gid,\n",
    "            \"choices\": r.get(\"choices\"),\n",
    "            \"nonsensical\": r.get(\"nonsensical\"),\n",
    "            \"sample_id\": r.get(\"sample_id\")\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84OHzAJaK4rr",
    "outputId": "0c6bac3f-2281-413e-d365-9e31d4964b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2280\n",
      "DEV: 588\n",
      "SYN: 792\n"
     ]
    }
   ],
   "source": [
    "# Load each split and convert into flat DataFrames for quick inspection.\n",
    "train_records = load_json_records(TRAIN_JSON_PATH)\n",
    "dev_records   = load_json_records(DEV_JSON_PATH)\n",
    "syn_records   = load_json_records(SYN_JSON_PATH)\n",
    "train_df = pd.DataFrame(build_examples(train_records))\n",
    "dev_df   = pd.DataFrame(build_examples(dev_records))\n",
    "syn_df   = pd.DataFrame(build_examples(syn_records))\n",
    "print(\"TRAIN:\", len(train_df))\n",
    "print(\"DEV:\", len(dev_df))\n",
    "print(\"SYN:\", len(syn_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q7Clew4JwvB",
    "outputId": "f01fe08f-23da-4004-bf60-db1c7d577950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Story:\\nThe old machine hummed in the corner o...   \n",
      "1  Story:\\nThe old machine hummed in the corner o...   \n",
      "2  Story:\\nThe old machine hummed in the corner o...   \n",
      "3  Story:\\nThe old machine hummed in the corner o...   \n",
      "4  Story:\\nThe old machine hummed in the corner o...   \n",
      "\n",
      "                                             meaning  label     stdev  \\\n",
      "0  Meaning (intended definition):\\nthe difference...    3.0  1.581139   \n",
      "1  Meaning (intended definition):\\nthe inherent c...    3.8  0.836660   \n",
      "2  Meaning (intended definition):\\nthe difference...    2.2  1.303840   \n",
      "3  Meaning (intended definition):\\nthe inherent c...    4.4  0.894427   \n",
      "4  Meaning (intended definition):\\nthe difference...    2.6  1.516575   \n",
      "\n",
      "                                         group_id          choices  \\\n",
      "0  potential||The potential couldn't be measured.  [4, 5, 2, 3, 1]   \n",
      "1  potential||The potential couldn't be measured.  [5, 3, 4, 4, 3]   \n",
      "2  potential||The potential couldn't be measured.  [2, 1, 4, 3, 1]   \n",
      "3  potential||The potential couldn't be measured.  [4, 5, 5, 3, 5]   \n",
      "4  potential||The potential couldn't be measured.  [1, 1, 4, 4, 3]   \n",
      "\n",
      "                           nonsensical sample_id  \n",
      "0  [False, False, False, False, False]      1843  \n",
      "1  [False, False, False, False, False]      1844  \n",
      "2  [False, False, False, False, False]      1845  \n",
      "3  [False, False, False, False, False]      1846  \n",
      "4  [False, False, False, False, False]      1847  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX9cjEyOHSgF",
    "outputId": "9068b590-9c39-4f96-c67b-7bf897e423ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 3072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Append synthetic supervision to strengthen rare-sense coverage.\n",
    "train_df = pd.concat([train_df, syn_df])\n",
    "print(\"TRAIN:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67pFeacjK4pV",
    "outputId": "ade16d8f-53a2-45fa-d68e-133206bf5056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2457 VAL: 615\n"
     ]
    }
   ],
   "source": [
    "# Keep 20% of the mix as a validation set for Trainer callbacks.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAIN:\", len(train_df), \"VAL:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "1adf3302b0fc44d9ae7b0a991c1bd755",
      "16500ef47111406283557dc63e72dc1c",
      "762ccb9d773f4bb7807d9f6d876f68f9",
      "3db94373f7b94e6889bea061aa73924b",
      "d60c03bfe8524d8d9656498155e93223",
      "b3760d2f62c843aeb47480e2288aa380",
      "d0b10b30c6074f61b7732a850b227c92",
      "3828debad0f64a0d82b6462ede0570dc",
      "30315ae9b01647568ee57aa40094b277",
      "6c7b9cdf46c24b6fa8bd9e61a219670c",
      "d6e2cbd89b6c4a9b8b0cd37875da71a9",
      "47ed158e8fad4cba8fb705a82983f7c0",
      "45956c9523a641dcaf11699fdaf6edc6",
      "48f69603f6c54332a6ab17341b1d804d",
      "ed92fcd691324a38a63e8c85bd6612a8",
      "1fb3b63efb704d8287e67a55b1581c7c",
      "4ac8d9fce83b4f38a21dd114778cf953",
      "a74a69b89bbc4312a9d2169c39c5727b",
      "f62d4f187e02432cbe2879cf958bf3c1",
      "f037bced4bd843f68ac56d40e6e91add",
      "349e8e8810a840bda926f03dc8b71351",
      "b4648e35faac4ac78d5121bfda0cc51b",
      "28aa592518c0485988765ef2da8700a8",
      "d06d851b8b8f4120a07316afbda5b1fb",
      "1d1f4cf6db50426489fac4e9fa1136f8",
      "a29ac2b8f6bf460da167d243a657409c",
      "d758d399b4d94e5cabbd9c3e52dec0b0",
      "bc825cbb81724f01b15643e107ba09ca",
      "a9e6347f91104e8a9bafaddd7a0e0ebd",
      "d9e36a9085534fca9189be93480d10e9",
      "7e19ef1303ab455fb1907e2b4169ce3f",
      "0793d6ab9e5a4952b8843b6f1c453a4d",
      "4aef748c59464fb0b8379657502d2936"
     ]
    },
    "id": "RjpSLkDhK4nB",
    "outputId": "3912c3db-e969-4f08-9849-e1e21f955955"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adf3302b0fc44d9ae7b0a991c1bd755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ed158e8fad4cba8fb705a82983f7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aa592518c0485988765ef2da8700a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    \"\"\"Tokenize the (story, definition) pair and attach regression labels.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch[\"context\"],\n",
    "        batch[\"meaning\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"label\"]\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "1adba6f494a041dfa1bfa35216e779ec",
      "8910535334f04d539fd3097554cb6b6d",
      "475759162815413b8005bcf24ed34c4d",
      "34a7cb3375094fcfb610a0b3f2883e6a",
      "54adff45c7dc4a7fabdf0a5bfb1577f6",
      "6b1c5ece9fb6487e9e8011e579d9c6e6",
      "fcb6f946398e47a7b15dff72106ebbd2",
      "54f5b3e6dd6146eaa11bc2a8aaa8fe6c",
      "fecc6778e7c54284b34788d3c142143f",
      "23886fdadcd64925bc571024d9e508af",
      "07156c4a97654239b6f275cf92e3a863",
      "b6f86eae4d684333a13d125e491167f1",
      "d2eb9a1c44e6480b98e587a1170495ad",
      "20f31bd828a54f76a82f3f20be4c55cf",
      "9e6e8f8fca7d450990437ea4474ad071",
      "3a06258c119446fd8e7c1142a3743353",
      "d5dffe82f6f44ccb986407cf8fb4052d",
      "f772e71b07a245b0bdb26aa7b641e736",
      "c0b2fcdafcb149fbaa2aedfcf02d6cd6",
      "b5fe095ececb4d0787f95fce316e302e",
      "f4aaead3fe524d49b7b6ec5b0271d05a",
      "eec23da3b963408fb93cfe86b5d889e7",
      "48b22c5daf474210a3a9c3df19081942",
      "621701f307be4a6f909f7dcfe11e6392",
      "4ff4b9969b4646c39aa1bddc0c36bc36",
      "daef5548cb55482e935f34aeff43d49b",
      "68974e3ed8f948868eeb12bf15f30970",
      "62470141138742e889b52736ffc589f3",
      "d5aa44628b774c4d8bea7967afaec82c",
      "b35a4939f65a45869d1823d33fb41e10",
      "6c4ec173679a40b884f3f4355f7b8b2b",
      "bf3deb11c1744c12914f36ccf0b5d96f",
      "476927bc759b4d61850b6ce887049b03"
     ]
    },
    "id": "u7B64q6ELAhf",
    "outputId": "4cad2857-a3a8-4ffb-ce03-001c262f366b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adba6f494a041dfa1bfa35216e779ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f86eae4d684333a13d125e491167f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b22c5daf474210a3a9c3df19081942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Materialize Hugging Face datasets once to avoid recomputing tokens.\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "val_dataset   = Dataset.from_pandas(val_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df, preserve_index=False).map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "fd679d01421243f49a180b179cfbfdde",
      "3829b5afb7a5468bae8e715852df2024",
      "89b199a518664b2c8c6b27bc4198c543",
      "be0ea3bf276243f2a3ccf909d4fbae62",
      "269248b013f14985baba54ff8a26816d",
      "a2374bee1d0340b1a181803f2b4ac033",
      "b651521d0e14410aa0c22734ab0be365",
      "4ee1f3a1fbde402f805269f698949409",
      "50b9ff173ef7425dba2a8783fb63bc66",
      "11c6d7018546423cba4c13e6b4f35273",
      "e0ee55c523464b67976e6aa81c16b44e"
     ]
    },
    "id": "M9ZJQKEALATb",
    "outputId": "83656378-2f08-4933-969e-b8b6afd137d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd679d01421243f49a180b179cfbfdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKw-lMXRLARn"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Track Spearman (ranking quality) and MAE (absolute error).\"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.reshape(-1)\n",
    "\n",
    "    sp = spearmanr(labels, preds).correlation\n",
    "    mae = np.mean(np.abs(labels - preds))\n",
    "\n",
    "    return {\"spearman\": float(sp), \"mae\": float(mae)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5HUW9saLIt-"
   },
   "outputs": [],
   "source": [
    "# DeBERTa-large finetuning setup that fits comfortably on a Colab GPU.\n",
    " args = TrainingArguments(\n",
    "    output_dir=\"./tmp_deberta_regression\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEf0dpC-LIqV",
    "outputId": "30a38e83-0992-4762-92da-7c8a767e8a91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3517603358.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495,
     "referenced_widgets": [
      "382129509eb4420398a824c2bcbafb6e",
      "3492c8570d244df4bf82ffe1c154d0f6",
      "29a91726061e4231a8b9a2b6b7374f01",
      "ee161a59e106411298a6d0d5b8bc915b",
      "d93717429b7e4dd7b2d9141c5ccb8e01",
      "ede004d878244a06be1fe401d329ffc4",
      "e3c4531007754c34999448bf9016b855",
      "03864eab950e4ac8aed6ccd7036a261d",
      "4223c062959f43c08727eaca55f03c39",
      "c72e6de11a7a493ab7cbce5a6e4d25e4",
      "6a3e86d382504697952e63602fbaa4c8"
     ]
    },
    "id": "b9j3fAOmLLQP",
    "outputId": "e9aea978-903c-4005-b45a-bc8e2ce47153"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382129509eb4420398a824c2bcbafb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1540' max='1540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1540/1540 05:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.312800</td>\n",
       "      <td>1.137635</td>\n",
       "      <td>0.410090</td>\n",
       "      <td>0.889729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.788300</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.635107</td>\n",
       "      <td>0.744595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.764535</td>\n",
       "      <td>0.675281</td>\n",
       "      <td>0.665886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.745632</td>\n",
       "      <td>0.687502</td>\n",
       "      <td>0.646920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.731655</td>\n",
       "      <td>0.702501</td>\n",
       "      <td>0.650550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.779529</td>\n",
       "      <td>0.701392</td>\n",
       "      <td>0.660222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.805003</td>\n",
       "      <td>0.696618</td>\n",
       "      <td>0.689995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.714749</td>\n",
       "      <td>0.701230</td>\n",
       "      <td>0.641983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.708533</td>\n",
       "      <td>0.700396</td>\n",
       "      <td>0.640737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.740617</td>\n",
       "      <td>0.700923</td>\n",
       "      <td>0.658683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1540, training_loss=0.440579084142462, metrics={'train_runtime': 319.4472, 'train_samples_per_second': 76.914, 'train_steps_per_second': 4.821, 'total_flos': 1.144881540601344e+16, 'train_loss': 0.440579084142462, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "SWY7VsQNLLNv",
    "outputId": "f5cf3bf6-d85d-425a-bea4-4af5f6b3f9ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEV EVALUATION ===\n",
      "Spearman: 0.5949\n",
      "MAE:      0.7918\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(dev_dataset).predictions.reshape(-1)\n",
    "dev_labels = dev_df[\"label\"].to_numpy()\n",
    "\n",
    "global_spearman = spearmanr(dev_labels, preds).correlation\n",
    "mae = np.mean(np.abs(dev_labels - preds))\n",
    "\n",
    "print(f\"\\n=== DEV EVALUATION ===\")\n",
    "print(f\"Spearman: {global_spearman:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEg2ZzvSN7j2"
   },
   "outputs": [],
   "source": [
    "def bootstrap_test_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_stdev,\n",
    "    groups,\n",
    "    n_bootstrap=1000,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"Bootstrap confidence intervals to understand metric variance.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y_true)\n",
    "\n",
    "    global_sps = []\n",
    "    macro_sps = []\n",
    "    maes = []\n",
    "    acc_stdevs = []\n",
    "\n",
    "    group_to_indices = defaultdict(list)\n",
    "    for i, gid in enumerate(groups):\n",
    "        group_to_indices[gid].append(i)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample indices with replacement to mimic new dev draws.\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "\n",
    "        bt_true = y_true[idx]\n",
    "        bt_pred = y_pred[idx]\n",
    "        bt_stdev = y_stdev[idx]\n",
    "        bt_groups = [groups[i] for i in idx]\n",
    "\n",
    "        # ---- Global Spearman ----\n",
    "        rho = spearmanr(bt_true, bt_pred).correlation\n",
    "        global_sps.append(rho)\n",
    "\n",
    "        # ---- MAE ----\n",
    "        maes.append(np.mean(np.abs(bt_true - bt_pred)))\n",
    "\n",
    "        # ---- Accuracy within stdev ----\n",
    "        acc_stdevs.append(np.mean(np.abs(bt_true - bt_pred) <= bt_stdev))\n",
    "\n",
    "        # ---- Macro Spearman ----\n",
    "        local_group_map = defaultdict(list)\n",
    "        for i, g in enumerate(bt_groups):\n",
    "            local_group_map[g].append(i)\n",
    "\n",
    "        group_corrs = []\n",
    "        for g, idxs in local_group_map.items():\n",
    "            gt = bt_true[idxs]\n",
    "            gp = bt_pred[idxs]\n",
    "\n",
    "            if np.all(gt == gt[0]):\n",
    "                continue\n",
    "\n",
    "            c = spearmanr(gt, gp).correlation\n",
    "            if not np.isnan(c):\n",
    "                group_corrs.append(c)\n",
    "\n",
    "        if group_corrs:\n",
    "            macro_sps.append(np.mean(group_corrs))\n",
    "        else:\n",
    "            macro_sps.append(np.nan)\n",
    "\n",
    "    def summarize(arr):\n",
    "        # Emit per-metric means plus 95% percentile-based bounds.\n",
    "        arr = np.array(arr, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.nanmean(arr)),\n",
    "            \"ci_low\": float(np.nanpercentile(arr, 2.5)),\n",
    "            \"ci_high\": float(np.nanpercentile(arr, 97.5)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"global_spearman\": summarize(global_sps),\n",
    "        \"macro_spearman\": summarize(macro_sps),\n",
    "        \"mae\": summarize(maes),\n",
    "        \"acc_within_stdev\": summarize(acc_stdevs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw7WwJuYOAsf",
    "outputId": "5a18b5e6-8a81-4fff-a142-c6c7d255b946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOOTSTRAP RESULTS (TEST SET)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3268535183.py:52: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  c = spearmanr(gt, gp).correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_spearman     : 0.5944 [0.5332, 0.6502]\n",
      "macro_spearman      : 0.5643 [0.4746, 0.6507]\n",
      "mae                 : 0.7923 [0.7407, 0.8416]\n",
      "acc_within_stdev    : 0.6156 [0.5782, 0.6548]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBOOTSTRAP RESULTS (TEST SET)\")\n",
    "\n",
    "bootstrap_results = bootstrap_test_metrics(\n",
    "    y_true=dev_df[\"label\"].to_numpy(),\n",
    "    y_pred=preds,\n",
    "    y_stdev=dev_df[\"stdev\"].to_numpy(),\n",
    "    groups=dev_df[\"group_id\"].tolist(),\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "\n",
    "for metric, stats in bootstrap_results.items():\n",
    "    print(\n",
    "        f\"{metric:20s}: \"\n",
    "        f\"{stats['mean']:.4f} \"\n",
    "        f\"[{stats['ci_low']:.4f}, {stats['ci_high']:.4f}]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
