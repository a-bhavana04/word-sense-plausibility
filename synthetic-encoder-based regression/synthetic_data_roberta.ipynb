{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQGHrD4GLd6B",
    "outputId": "53b605a7-33af-463e-abb0-f3af185279dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aT5qZq2pFBGH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0cT_KkUKype"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"Keep every source of randomness in sync for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "H_20VJeqKynI"
   },
   "outputs": [],
   "source": [
    "TRAIN_JSON_PATH = \"/content/drive/MyDrive/nlp/train.json\"\n",
    "DEV_JSON_PATH   = \"/content/drive/MyDrive/nlp/dev.json\"\n",
    "SYN_JSON_PATH   = \"/content/drive/MyDrive/nlp/merged_updated.json\"\n",
    "MODEL_NAME = \"FacebookAI/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zuh4fV-KylF"
   },
   "outputs": [],
   "source": [
    "def load_json_records(path, key=None):\n",
    "    \"\"\"Read a JSON file from disk and optionally select a nested key.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if key:\n",
    "        data = data[key]\n",
    "    return list(data.values()) if isinstance(data, dict) else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Oq_qFlQK4tw"
   },
   "outputs": [],
   "source": [
    "def build_examples(records):\n",
    "    \"\"\"Transform nested JSON annotations into flat training examples.\"\"\"\n",
    "    # Normalize: flatten nested lists so every item is a dict\n",
    "    def flatten(xs):\n",
    "        for x in xs:\n",
    "            if isinstance(x, list):\n",
    "                yield from flatten(x)\n",
    "            else:\n",
    "                yield x\n",
    "\n",
    "    # If a single dict is passed\n",
    "    if isinstance(records, dict):\n",
    "        records = [records]\n",
    "\n",
    "    # Flatten any nested structure\n",
    "    records = list(flatten(records))\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for r in records:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "\n",
    "        pre = (r.get(\"precontext\") or \"\").strip()\n",
    "        sent = (r.get(\"sentence\") or \"\").strip()\n",
    "        end = (r.get(\"ending\") or \"\").strip()\n",
    "        meaning = (r.get(\"judged_meaning\") or \"\").strip()\n",
    "\n",
    "        meaning_txt = (\n",
    "            \"Meaning (intended definition):\\n\"\n",
    "            f\"{meaning}\"\n",
    "        )\n",
    "\n",
    "        # Concatenate the narrative snippets into a coherent block.\n",
    "        story_parts = [pre, sent]\n",
    "        if end:\n",
    "            story_parts.append(end)\n",
    "\n",
    "        story_txt = \"Story:\\n\" + \"\\n\".join(story_parts)\n",
    "\n",
    "        ex_sent = (r.get(\"example_sentence\") or \"\").strip()\n",
    "        if ex_sent:\n",
    "            story_txt += f\"\\nExample sentence: {ex_sent}\"\n",
    "\n",
    "        label = float(r.get(\"average\", 0.0))\n",
    "\n",
    "        gid = f\"{r.get('homonym', '')}||{sent}\"\n",
    "\n",
    "        # Keep auxiliary metadata for downstream stats and debugging.\n",
    "        out.append({\n",
    "            \"context\": story_txt,\n",
    "            \"meaning\": meaning_txt,\n",
    "            \"label\": label,\n",
    "            \"stdev\": float(r.get(\"stdev\", 0)),\n",
    "            \"group_id\": gid,\n",
    "            \"choices\": r.get(\"choices\"),\n",
    "            \"nonsensical\": r.get(\"nonsensical\"),\n",
    "            \"sample_id\": r.get(\"sample_id\")\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84OHzAJaK4rr",
    "outputId": "656370ab-3711-4ba5-cebf-9844f5f50f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2280\n",
      "DEV: 588\n",
      "SYN: 792\n"
     ]
    }
   ],
   "source": [
    "# Flatten each JSON split into DataFrames so we can concatenate later.\n",
    "train_records = load_json_records(TRAIN_JSON_PATH)\n",
    "dev_records   = load_json_records(DEV_JSON_PATH)\n",
    "syn_records   = load_json_records(SYN_JSON_PATH)\n",
    "train_df = pd.DataFrame(build_examples(train_records))\n",
    "dev_df   = pd.DataFrame(build_examples(dev_records))\n",
    "syn_df   = pd.DataFrame(build_examples(syn_records))\n",
    "print(\"TRAIN:\", len(train_df))\n",
    "print(\"DEV:\", len(dev_df))\n",
    "print(\"SYN:\", len(syn_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4q7Clew4JwvB",
    "outputId": "2e063168-ffe9-4c45-c4af-8a04fe936786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             context  \\\n",
      "0  Story:\\nThe old machine hummed in the corner o...   \n",
      "1  Story:\\nThe old machine hummed in the corner o...   \n",
      "2  Story:\\nThe old machine hummed in the corner o...   \n",
      "3  Story:\\nThe old machine hummed in the corner o...   \n",
      "4  Story:\\nThe old machine hummed in the corner o...   \n",
      "\n",
      "                                             meaning  label     stdev  \\\n",
      "0  Meaning (intended definition):\\nthe difference...    3.0  1.581139   \n",
      "1  Meaning (intended definition):\\nthe inherent c...    3.8  0.836660   \n",
      "2  Meaning (intended definition):\\nthe difference...    2.2  1.303840   \n",
      "3  Meaning (intended definition):\\nthe inherent c...    4.4  0.894427   \n",
      "4  Meaning (intended definition):\\nthe difference...    2.6  1.516575   \n",
      "\n",
      "                                         group_id          choices  \\\n",
      "0  potential||The potential couldn't be measured.  [4, 5, 2, 3, 1]   \n",
      "1  potential||The potential couldn't be measured.  [5, 3, 4, 4, 3]   \n",
      "2  potential||The potential couldn't be measured.  [2, 1, 4, 3, 1]   \n",
      "3  potential||The potential couldn't be measured.  [4, 5, 5, 3, 5]   \n",
      "4  potential||The potential couldn't be measured.  [1, 1, 4, 4, 3]   \n",
      "\n",
      "                           nonsensical sample_id  \n",
      "0  [False, False, False, False, False]      1843  \n",
      "1  [False, False, False, False, False]      1844  \n",
      "2  [False, False, False, False, False]      1845  \n",
      "3  [False, False, False, False, False]      1846  \n",
      "4  [False, False, False, False, False]      1847  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX9cjEyOHSgF",
    "outputId": "064071cc-1ace-45b2-ceba-cbbe46a4c9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 3072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Augment with synthetic annotations before the train/val split.\n",
    "train_df = pd.concat([train_df, syn_df])\n",
    "print(\"TRAIN:\", len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67pFeacjK4pV",
    "outputId": "4d80adce-26fd-48bf-e169-b1c9124f4b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2457 VAL: 615\n"
     ]
    }
   ],
   "source": [
    "# Reserve out 20% for validation monitoring.\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAIN:\", len(train_df), \"VAL:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "2b760e131d3a4ca3b495e98c0d587496",
      "056fce28736b46da95120c6ba762e21c",
      "c06adcaa8c7141838d6569d35202dced",
      "1810090a56fe41b6b37d425622fffa5d",
      "85cab840b5c24fff8d78065852098f25",
      "4326eed51e1f4e4c94217c9dfff21bd2",
      "5e77a7e7e9a449b6a27d987c9fee104d",
      "752a514e6ea74d0c9d58e9c24da6e088",
      "0d6c0887d3f94602a518e4f22eef99b5",
      "8958ccc2e2654bf7ac9dad184734874c",
      "60902d8cc4444464b1f271cce4a9e128",
      "af8edf1fb990491987bae609d1d256b5",
      "4dac6fd2f36e4b10bd342a7ed374c290",
      "b9ed023365c043ae8005c395e101172b",
      "97cd26ec280f45f1b66acf8696786c98",
      "ec1d177434c649ba99d3c7cf3b2e9173",
      "3547d822b687460594bb899f42cf4d73",
      "8cc054cb8a71437daa747134995350bc",
      "b6087048f019484a922f01402ef09158",
      "2995ee8f9e784db1ae3c3e361806ef9f",
      "607eeecb42eb4455836b2f5aefc5eced",
      "09cb413804ca4d10a222ee27d63d022c",
      "a409d61f943f489188960cf923932946",
      "8d9271a64b234fc1a247a841a1b1be26",
      "0915fa47dd3a47d29efe1319eec1081a",
      "a0e31a1f10e74b4ca5d881aa8e8754f5",
      "08313141ed32452a91a61f142005d26d",
      "9d7aa3fe7d4b4b238004ac3de06e9469",
      "1f86a530d9194c08a5baa4c69adbb1c0",
      "b63100266eab4a1a8d4295c748034757",
      "a48abc500cf4475498ece930cfc6f148",
      "24855212c1f443618abc0adf6eaddc60",
      "7b0a0e24b7c54536a09f8a7b08d27543",
      "13afcb8c45e94d46bcb81216c4973601",
      "4cf49b6bb6504d29ad498e7cf9a38909",
      "bb5be83b513f4f3e97a5df9c0069916c",
      "7180cf4a60a24f838cb726923a70d81d",
      "d39cde0900e64ef8ad721f25003ae014",
      "8f554e6a44844d1c9f8f4049015bdd7c",
      "5a3b72509e9c4a018e604e0ee2837499",
      "1d4af6abb2e2411984c2fcfa7b4b0124",
      "edeada8e717b42468af9a1f49d7d589d",
      "75a1bc4c25fc4e0d9e995b7d9f25ac64",
      "1e933622f95c4714b65aa8989c86742f",
      "229276134de4476b9b4336c26dfe93a0",
      "2b97bf40f276425f8b099621d80ea334",
      "07c137e5355e4f29921af177437ef32e",
      "a1783cac589a45178341c728ed0970e1",
      "e6ea456f670443768189011bd62e3ef7",
      "9afb3547b38e46e99bbd50afadd6ae3b",
      "8d13ca6573ab469e8d9925e93792dc53",
      "c56fdc3103b64f2abeb1d7d4c7ad00c8",
      "693ef3a1d32546f0adf1b3438169e727",
      "779d9668d9dd42b4a6168cc27bc0aa6e",
      "b06ddc8baa21429c923d8461b70547a2"
     ]
    },
    "id": "RjpSLkDhK4nB",
    "outputId": "5b3ee855-4a8b-495d-b8db-871dacbe87f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b760e131d3a4ca3b495e98c0d587496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8edf1fb990491987bae609d1d256b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a409d61f943f489188960cf923932946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13afcb8c45e94d46bcb81216c4973601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229276134de4476b9b4336c26dfe93a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    \"\"\"Tokenize the paired inputs once and stash the regression label.\"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch[\"context\"],\n",
    "        batch[\"meaning\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"label\"]\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "f46e033faaff48df986f5c6fc89b9863",
      "0af557e02d2a4300b95b6e4d2bae6ad0",
      "401848d17a5944d0ad22cd0c82d6283d",
      "344db64e29e34cadb9948c96eb26f4f7",
      "827c373c0f334bf289dce6218dca8a90",
      "de8de2ca1415405a9dcfa1397064f662",
      "34796cd2aa264a90b731e23c14611d25",
      "a827a70b5ab94852a9a1f9214f8b2b42",
      "ccd408784d3d48809b9045148d98f51e",
      "310ea03bc45a46818570afed9ce51f3c",
      "038cd4e7f868487c81c88316f1d94e85",
      "845a0c3691de4fb4b21a6f053c8a9058",
      "b4087a81bcc94626a1654e94b9a7730c",
      "e73490b7b954450a8dc1dc7e79b8111d",
      "c9273990481944f6aa2fdd1b312347ee",
      "8d231a7a8c704df9869e936346dca5ad",
      "20874fd5be5c4cebba8919e3b762c1bb",
      "41a0ee7fb1df4f9cb0cc961ab60f877c",
      "ccaa49223d21492daf36f0ee2c4e1118",
      "0081569e8740407f8a60bfb5ac62bf46",
      "3fee79ccde8747d39bab6a2180eac5c7",
      "5a558be634904470ad097e6cb00dcae8",
      "35bb0c820a564b009ae41d3e3b05120b",
      "3c21a95fafdf40ff8656f3fa284f03e9",
      "909566dec2b148c7933074f452c0b001",
      "fb64d176517b49d1afd7842ddb9f5e89",
      "63326ae2ffbf405093433d623802f6ed",
      "7e0c1fba409a453cb918693178c63615",
      "ddbc9289b2704217897b254d70db959d",
      "2f7ee3ee7578460182777a2251172f06",
      "8f3c9fda71bd4ea8b6a70d1685fcba5b",
      "11074847d21a4dc58db5ec9c486119c3",
      "295b570a3f06472faf3eafb3f391e333"
     ]
    },
    "id": "u7B64q6ELAhf",
    "outputId": "d7f40b37-4e9b-4cba-bc54-28fcacb4f70b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46e033faaff48df986f5c6fc89b9863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845a0c3691de4fb4b21a6f053c8a9058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bb0c820a564b009ae41d3e3b05120b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cache tokenized datasets so the Trainer works with arrow batches directly.\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "val_dataset   = Dataset.from_pandas(val_df, preserve_index=False).map(tokenize_batch, batched=True)\n",
    "dev_dataset   = Dataset.from_pandas(dev_df, preserve_index=False).map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "d6547aea39fd4617adc449e9d0733acf",
      "b3bcdd9b35fb498eab6aaa0109cf62d2",
      "8295638d75634c819b42e955873a3641",
      "f4a0d554965b44abb4738eea5ceeab62",
      "f1aeb80719ba4a498daaca197ebae245",
      "3b9bc4c952eb44a1b25c4928dbc3c9f2",
      "be1142b91e4b4cfab48bc102581c3dff",
      "d55f84f42e804856a1d0982216aa9318",
      "6f6147c052d045599f2caf2653be11d5",
      "c00e3eb5b6c94721a9a586d62e0aaf5a",
      "bac0c3dca76e429889b5133526c4ac95"
     ]
    },
    "id": "M9ZJQKEALATb",
    "outputId": "b917ab63-1d03-4dc3-bfe3-cf488bfe27f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6547aea39fd4617adc449e9d0733acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKw-lMXRLARn"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute ranking fidelity (Spearman) and scale error (MAE).\"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.reshape(-1)\n",
    "\n",
    "    sp = spearmanr(labels, preds).correlation\n",
    "    mae = np.mean(np.abs(labels - preds))\n",
    "\n",
    "    return {\"spearman\": float(sp), \"mae\": float(mae)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5HUW9saLIt-"
   },
   "outputs": [],
   "source": [
    "# Baseline hyperparameters for adapting RoBERTa-base to score plausibility.\n",
    " args = TrainingArguments(\n",
    "    output_dir=\"./tmp_roberta_regression\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEf0dpC-LIqV",
    "outputId": "f76171a1-a7b9-403c-bac1-bf3691d7221e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3517603358.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "b9j3fAOmLLQP",
    "outputId": "373cf189-e922-4594-bb87-c4ebd165c0c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1540' max='1540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1540/1540 01:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>1.176950</td>\n",
       "      <td>0.297987</td>\n",
       "      <td>0.945746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>0.979180</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.821847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.761800</td>\n",
       "      <td>1.114021</td>\n",
       "      <td>0.604398</td>\n",
       "      <td>0.824782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.562900</td>\n",
       "      <td>0.917337</td>\n",
       "      <td>0.622885</td>\n",
       "      <td>0.738365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.454400</td>\n",
       "      <td>0.901285</td>\n",
       "      <td>0.658927</td>\n",
       "      <td>0.718928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.905752</td>\n",
       "      <td>0.658384</td>\n",
       "      <td>0.718758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.862233</td>\n",
       "      <td>0.672494</td>\n",
       "      <td>0.706680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.886104</td>\n",
       "      <td>0.675412</td>\n",
       "      <td>0.707214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.816339</td>\n",
       "      <td>0.676194</td>\n",
       "      <td>0.677677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.874953</td>\n",
       "      <td>0.677571</td>\n",
       "      <td>0.701018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1540, training_loss=0.6079686257746313, metrics={'train_runtime': 86.0657, 'train_samples_per_second': 285.48, 'train_steps_per_second': 17.893, 'total_flos': 3232290293406720.0, 'train_loss': 0.6079686257746313, 'epoch': 10.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "SWY7VsQNLLNv",
    "outputId": "80028034-a0d4-4a01-cb77-d7854d526ce7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEV EVALUATION ===\n",
      "Spearman: 0.4280\n",
      "MAE:      0.9774\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(dev_dataset).predictions.reshape(-1)\n",
    "dev_labels = dev_df[\"label\"].to_numpy()\n",
    "\n",
    "global_spearman = spearmanr(dev_labels, preds).correlation\n",
    "mae = np.mean(np.abs(dev_labels - preds))\n",
    "\n",
    "print(f\"\\n=== DEV EVALUATION ===\")\n",
    "print(f\"Spearman: {global_spearman:.4f}\")\n",
    "print(f\"MAE:      {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEg2ZzvSN7j2"
   },
   "outputs": [],
   "source": [
    "def bootstrap_test_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_stdev,\n",
    "    groups,\n",
    "    n_bootstrap=1000,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"Quantify metric uncertainty with simple percentile bootstrapping.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y_true)\n",
    "\n",
    "    global_sps = []\n",
    "    macro_sps = []\n",
    "    maes = []\n",
    "    acc_stdevs = []\n",
    "\n",
    "    group_to_indices = defaultdict(list)\n",
    "    for i, gid in enumerate(groups):\n",
    "        group_to_indices[gid].append(i)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Draw n examples with replacement to simulate alternate test sets.\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "\n",
    "        bt_true = y_true[idx]\n",
    "        bt_pred = y_pred[idx]\n",
    "        bt_stdev = y_stdev[idx]\n",
    "        bt_groups = [groups[i] for i in idx]\n",
    "\n",
    "        # ---- Global Spearman ----\n",
    "        rho = spearmanr(bt_true, bt_pred).correlation\n",
    "        global_sps.append(rho)\n",
    "\n",
    "        # ---- MAE ----\n",
    "        maes.append(np.mean(np.abs(bt_true - bt_pred)))\n",
    "\n",
    "        # ---- Accuracy within stdev ----\n",
    "        acc_stdevs.append(np.mean(np.abs(bt_true - bt_pred) <= bt_stdev))\n",
    "\n",
    "        # ---- Macro Spearman ----\n",
    "        local_group_map = defaultdict(list)\n",
    "        for i, g in enumerate(bt_groups):\n",
    "            local_group_map[g].append(i)\n",
    "\n",
    "        group_corrs = []\n",
    "        for g, idxs in local_group_map.items():\n",
    "            gt = bt_true[idxs]\n",
    "            gp = bt_pred[idxs]\n",
    "\n",
    "            if np.all(gt == gt[0]):\n",
    "                continue\n",
    "\n",
    "            c = spearmanr(gt, gp).correlation\n",
    "            if not np.isnan(c):\n",
    "                group_corrs.append(c)\n",
    "\n",
    "        if group_corrs:\n",
    "            macro_sps.append(np.mean(group_corrs))\n",
    "        else:\n",
    "            macro_sps.append(np.nan)\n",
    "\n",
    "    def summarize(arr):\n",
    "        # Retain the mean plus the 2.5/97.5 percentiles as the CI bounds.\n",
    "        arr = np.array(arr, dtype=float)\n",
    "        return {\n",
    "            \"mean\": float(np.nanmean(arr)),\n",
    "            \"ci_low\": float(np.nanpercentile(arr, 2.5)),\n",
    "            \"ci_high\": float(np.nanpercentile(arr, 97.5)),\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"global_spearman\": summarize(global_sps),\n",
    "        \"macro_spearman\": summarize(macro_sps),\n",
    "        \"mae\": summarize(maes),\n",
    "        \"acc_within_stdev\": summarize(acc_stdevs),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iw7WwJuYOAsf",
    "outputId": "06ec2441-bc5b-4441-91a5-3d3cdffbc696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOOTSTRAP RESULTS (TEST SET)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3268535183.py:52: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  c = spearmanr(gt, gp).correlation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_spearman     : 0.4282 [0.3569, 0.4964]\n",
      "macro_spearman      : 0.4109 [0.3202, 0.5078]\n",
      "mae                 : 0.9776 [0.9198, 1.0379]\n",
      "acc_within_stdev    : 0.5482 [0.5068, 0.5901]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBOOTSTRAP RESULTS (TEST SET)\")\n",
    "\n",
    "bootstrap_results = bootstrap_test_metrics(\n",
    "    y_true=dev_df[\"label\"].to_numpy(),\n",
    "    y_pred=preds,\n",
    "    y_stdev=dev_df[\"stdev\"].to_numpy(),\n",
    "    groups=dev_df[\"group_id\"].tolist(),\n",
    "    n_bootstrap=1000\n",
    ")\n",
    "\n",
    "\n",
    "for metric, stats in bootstrap_results.items():\n",
    "    print(\n",
    "        f\"{metric:20s}: \"\n",
    "        f\"{stats['mean']:.4f} \"\n",
    "        f\"[{stats['ci_low']:.4f}, {stats['ci_high']:.4f}]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
